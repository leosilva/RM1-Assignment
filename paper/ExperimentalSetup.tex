\section{Experimental Setup}

In this section, we present the setup of this study. We first state our problem, then define the independent and dependent variables. After that, we show the hypothesis and describe our dataset, showing all its characteristics.

\subsection{Problem Statement}

As introduced in the first section of this paper, sorting is a fundamental concept and essential for solving other problems. The content of memory location can change unexpectedly, i.e., faults may happen at any time. Considering this, the main objective of this work is to design experiments to answer the following question: \textit{How are sorting algorithms affected by memory faults?}

\subsection{Variables}

For this experimental study, we assume that the independent and dependent variables are as shown in Table \ref{table-independent-variables} and Table \ref{table-dependent-variables} below:

\begin{table}[H]
    \caption{Independent variables.}
    \begin{center}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Variable} & \textbf{Description} \\
    \hline
    Probability of failure & Probability of a fault to occur \\
    \hline
    Array size & Size of the array of integers to be sorted \\
    \hline
    Sorting algorithm & Algorithm used to sort the array \\
    \hline
    \end{tabular}
    \label{table-independent-variables}
    \end{center}
\end{table}

\begin{table}[H]
    \caption{Dependent variables.}
    \begin{center}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Variable} & \textbf{Description} \\
    \hline
    Largest subarray size & Size of the largest sorted subarray produced under the memory fault \\
    \hline
    Percentage of largest subarray size & Percentage of \textit{largest subarray size} related to \textit{array size} independent variable \\
    \hline
    Unordered elements quantity & Quantity of elements out of position after sorting algorithm execution. Adapted of the \\ & \textit{k-unordered sequence} measure of disorder defined in \cite{Ferraro-Petrillo2009} \\
    \hline
    Percentage of unordered elements quantity & Percentage of \textit{unordered elements quantity} related to \textit{array size} independent variable \\
    \hline
    \end{tabular}
    \label{table-dependent-variables}
    \end{center}
\end{table}

\subsection{Hypothesis}

The set of hypothesis defined to test and draw some conclusions about this experiment are listed below. The confidence degree defined for hypothesis testing was 95\% ($\alpha = 0.05$ and $\alpha - 1 = 0.95$).

\begin{itemize}
    \item \textbf{Hypothesis 1:} For a given probability of failure and array size, tested algorithms will produce a different percentage of unordered elements quantity.
    \item \textbf{Hypothesis 2:} For a given probability of failure and array size, tested algorithms will produce a different percentage of the largest subarray size.
    \item \textbf{Hypothesis 3:} For each algorithm, the array size and probability of failure have a significative impact on the percentage of unordered elements quantity.
    \item \textbf{Hypothesis 4:} For each algorithm, the array size and probability of failure have a significative impact on the percentage of the largest subarray size.
\end{itemize}

\subsection{Dataset}

To conduct the proposed study, we define the values of the independent variables, as shown in Table \ref{table-independent-variables-values}:

\begin{table}[H]
    \caption{Values of the independent variables.}
    \begin{center}
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Variable} & \textbf{Values} \\
    \hline
    Probability of failure & 1\%, 2\% and 5\% \\
    \hline
    Array size & 100, 1000 and 10000 \\
    \hline
    Sorting algorithm & Bubblesort, Quicksort, Mergesort and Insertionsort \\
    \hline
    \end{tabular}
    \label{table-independent-variables-values}
    \end{center}
\end{table}

Based on these variables, we ran an existing script \textit{gen.py} to produce input files. We define that our sample was composed by 30 input files for a given combination of the probability of failure and array size. So, considering this, we ran 30 times for each combination of these independent variables, producing 30 inputs, totalizing 270 files. Figure \ref{fig-input-file-example} shows an example of produced input files:

\begin{verbbox}[\mbox{}]
0.01 100 9 48 37 6 26 7 24 44 17 50 48 30 49 33 22 13 42 29 39 13 19 13 9 28 
34 1 33 27 14 45 48 40 11 17 6 50 9 44 20 16 37 45 23 14 38 29 10 49 44 46 35
45 15 2 22 1 46 40 8 48 23 23 32 35 3 15 8 36 17 24 27 48 28 5 28 50 44 4 25 
6 9 1 11 44 26 50 44 12 7 20 30 20 37 20 6 8 13 15 20 49
\end{verbbox}

\begin{figure}[H]
    \centering
    \fbox{
    \theverbbox
    }
    \caption{Example of input file.}
    \label{fig-input-file-example}
\end{figure}

The input data shown in the Figure \ref{fig-input-file-example} is divided as follows:

\begin{itemize}
    \item \textit{Probability of Failure}: the first number of the sequence (\texttt{0.01}) is the probability of memory failure when sorting;
    \item \textit{Sequence size}: the second number (\texttt{100}) means the size of the integers sequence used by sorting;
    \item \textit{Sequence}: the rest of the numbers represents a list of \textit{n} positive integers that will be sorted.
\end{itemize}

With this input data, we ran, for each one of these, all four algorithms considered in this study. The sorting algorithms used already existed. For example, using all 270 input files, we ran bubblesort, creating 270 output files, and so on for the other algorithms. At the end of executions, we get a total of 1080 output files. An output file looks like shown in Figure \ref{fig-output-file-example}:

\begin{verbbox}[\mbox{}]
[1]  9 48 37 6 26 7 24 44 17 50 48 30 49 33 22 13 42 29 39 13 19 13 9 28 34 1
33 27 14 45 48 40 11 17 6 50 9 44 20 16 37 45 23 14 38 29 10 49 44 46 35 45  
15 2 22 1 46 40 8 48 23 23 32 35 3 15 8 36 17 24 27 48 28 5 28 50 44 4 25 6 9
1 11 44 26 50 44 12 7 20 30 20 37 20 6 8 13 15 20 49
[2]  1 1 1 2 3 4 5 6 6 6 6 7 7 8 8 8 9 9 9 9 10 11 11 12 13 13 13 13 14 14 15 
15 15 16 17 17 17 19 20 20 20 20 22 22 23 23 23 24 24 25 26 26 27 27 28 28 28 
29 29 30 30 32 33 33 34 35 35 36 37 37 37 38 39 40 40 42 44 44 44 44 44 44 20 
45 45 45 46 46 48 48 48 48 48 49 49 50 49 50 50 50
[3]  1 1 1 2 3 4 5 6 6 6 6 7 7 8 8 8 9 9 9 9 10 11 11 12 13 13 13 13 14 14 15 
15 15 16 17 17 17 19 20 20 20 20 20 22 22 23 23 23 24 24 25 26 26 27 27 28 28 
28 29 29 30 30 32 33 33 34 35 35 36 37 37 37 38 39 40 40 42 44 44 44 44 44 44 
45 45 45 46 46 48 48 48 48 48 49 49 49 50 50 50 50
[4]  82
\end{verbbox}
    
\begin{figure}[H]
    \centering
    \fbox{
    \theverbbox
    }
    \caption{Example of output file.}
    \label{fig-output-file-example}
\end{figure}

The output file gives four essential data, as enumerated below:
\begin{itemize}
    \item \textit{1)} the original sequence of integers contained in the input file;
    \item \textit{2)} the sequence processed by the sorting algorithm under the memory fault model;
    \item \textit{3)} the sequence sorted correctly;
    \item \textit{4)} the size of the largest sorted subsequence in item \textit{2}. This number can be interpreted as a quality measure. As higher, most successful was the sorting operation.
\end{itemize}

After generating the dataset, we developed a Python program that reads the 1080 output files and produces a single CSV file (first lines showed in Figure \ref{fig-output-csv-file-example}). This file contains the columns listed below and was used to perform the exploratory data analysis and to run the statistical tests.
\begin{itemize}
    \item \textit{sorting\_algorithm}: the algorithm used to sort the array;
    \item \textit{probability\_of\_failure}: the probability of failure used when sorting;
    \item \textit{array\_size}: the size of the sorted array;
    \item \textit{largest\_subarray\_size}: the largest sorted subarray after sorting;
    \item \textit{unordered\_elements\_quantity}: quantity of unordered sequence elements after sorting.
    \item \textit{perc\_unordered\_elements\_quantity}: percentage of unordered elements after sorting related to original array;
    \item \textit{perc\_largest\_subarray\_size}: percentage of largest sorted subarray after sorting related to original array.
\end{itemize}

\begin{verbbox}[\mbox{}]
sorting_algorithm;probability_of_failure;array_size;largest_subarray_size;
unordered_elements_quantity;perc_unordered_elements_quantity;
perc_largest_subarray_size
quick;0.01;100;35;4;4.00;35.00
quick;0.01;100;36;8;8.00;36.00
quick;0.01;100;31;5;5.00;31.00
quick;0.01;100;20;6;6.00;20.00
quick;0.01;100;31;5;5.00;31.00
\end{verbbox}
        
\begin{figure}[H]
    \centering
    \fbox{
    \theverbbox
    }
    \caption{Example of output CSV file.}
    \label{fig-output-csv-file-example}
\end{figure}

We use Python libraries to make data analysis and plot graphs. These libraries were:
\begin{itemize}
    \item \textit{Pandas\footnote{https://pandas.pydata.org}}: open source library providing data structures and data analysis tools;
    \item \textit{NumPy\footnote{https://numpy.org}}: library for scientific computing with Python;
    \item \textit{SciPy\footnote{https://www.scipy.org}}: ecosystem of open-source software for mathematics, science, and engineering;
    \item \textit{StatsModels\footnote{https://www.statsmodels.org}}: module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration;
    \item \textit{Matplotlib\footnote{https://matplotlib.org}}: plotting library;
    \item \textit{Seaborn\footnote{https://seaborn.pydata.org}}: data visualization library based on matplotlib.
\end{itemize}